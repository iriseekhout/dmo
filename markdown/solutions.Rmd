---
title: "Solutions - Missing data workshop"
author: "dr. Iris Eekhout"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	comment = ">"
)
```

# Missing value analyses

## Solution 1: amount of missing values

```{R message=FALSE, warning=FALSE}
# load library mice to load boys data
library(mice)
```

**a. How many variables have missing data?**

All variables, except for age, have missing values, so in total 8 out of 9 variables have missing data.

```{r}
# summary to see which variables have missing data
summary(boys)
```

**b. How many rows in the data contain missing values?** 

In total 525 rows in the data have missing values, this is ~70%. 

```{r}
nic(boys)
nic(boys)/nrow(boys)
```

**c. How many overall matrix entries are missing? And how many observed?**

1622 matrix entries are missing and 5110 are observed; ~25% of the matrix entries are missing.

```{r}
sum(is.na(boys))
sum(!is.na(boys))

1622/(1622+5110)
```


## Solution 2: missing data patterns

**a. How many different missing data patterns occur in the data?**

14 patterns

```{r}
nrow(mice::md.pattern(boys, plot= T))

```


**b. What is the most frequently occurring pattern in the data?**

The pattern with "gen", "phb" and "tv" missing, occurs 437 times.

```{r}
mice::md.pattern(boys, plot= T)

```

**c. Looking at patterns that occur more than incidental (once or twice), which variables happen to be missing together often?**

Variables that are most often missing at the same time are "gen", "phb", and "tv". The patterns that occur more than once involve mostly all of these variables (pattern with "hc" and "gen", "pbh", "tv" 43 times and the pattern with "hgt", "bmi" and  "gen", "phb", "tv" 16 times, pattern "reg" and "gen", "phb", "tv", 3 times, pattern with "tv" mising 19 times).


**d. Inspect the missing data pairs. With what other variable(s) is height observed together with in more than half of the cases?**

The answer can be found looking at the first matrix `rr`. In the row of "hgt", the column values that are higher than 374 indicate variables that are observed with hgt more than half of the time: "age", "wgt", "bmi", "hc", and "reg".

```{r}
mice::md.pairs(boys)
```


## Solution 3: understanding missing data mechanisms

**a. What is the mean and standard deviation of knee pain score? And the association between BMI and knee pain (coefficient, standard error and p-value)?**

Mean= 14.81, sd=3.21; The association is significant with coefficient=0.35; se=0.14.

**b. What are the mean and standard deviation of the knee pain score? What is association between BMI and knee pain?**

Mean= 14.70, sd=3.24; The association is not significant (coefficient=0.33; se=0.18).

**c. How do these results compare to the complete data results?**

The mean and standard deviation have not changed much, however the power for the association was decreased which caused the association between BMI and knee pain to not be significant anymore. 

**d. What happens to the association between BMI and knee pain? Explain differences with the previous answer (sample size 100).**

At 0% missing: coefficient=0.57 and se=0.08 (significant); at 30% missing: coefficient=0.53 and se=0.10 (significant). The association does not change (much) and remains significant. The difference with answer c is explained by the change in sample size. Larger sample size, makes the analysis more robust against (MCAR) missing data.

**e. What is the association between BMI and knee pain? How does this compare to the association when the data were MCAR?**

There is a significant association with a coefficient of 0.49, se=0.11; the association is now less strong; the coefficient is lower (biased) (was 0.57 for 0% missing) and the standard error has increased slightly (was 0.08 for 0% missing).

**f. When there are 30% MAR missing data at sample size 250, at what BMI values do missing data on knee pain occur (inspect the scatterplot and the boxplots).**

More missing values at higher BMI values.

**g. Comparing the histograms, what knee pain values are mostly missing?**

In the MNAR situation, mostly the higher values of knee pain are missing. 

**h. What happens with the association between BMI and knee pain?**

* MCAR: coefficient= 0.46; se=0.10
* MAR: coefficient=0.37; se=0.11
* MNAR: coefficient=0.38; se=0.08 
* 0% missing: coefficient=0.54; se=0.07.

The association becomes less strong when you change from MCAR to MAR to MNAR, so coefficients get more biased. Also for MCAR the missings are nicely distributed over the BMI values. In the MAR mechanism there are more missings at higher values of BMI but also lower Knee Pain scores are missing. In the MNAR mechanism, mostly higher values of Knee Pain scores are missing.

**i What happens with the mean and standard deviation of the knee pain score?**

* MCAR: mean=14.79; sd=2.98 
* MAR: mean=14.01; sd=2.85
* MNAR: mean= 12.66; sd=2.21
* 0%missing: mean=14.77; sd=3.11

Both mean and standard deviation decrease when changing from MCAR to MAR to MNAR.


## Solution 4: evaluating the missing data mechanism

**a. Evaluate the missing data mechanism for the airquality data with univariate tests. What are your conclusions?**

Evaluation using T-tests for continuous variables and the Chi-square for categorical variables.


First create the missing data indicators for each variable with missing data.
```{r}
library(dplyr)
summary(airquality)
airqualitym <- airquality %>%
  #create missing data indicators
  mutate(ROzone = is.na(Ozone),
         RSolar.R = is.na(Solar.R))

```

Do a T-test for each missing data indicator with the continuous variables and a chi square test for the categorical variables. We investigate Month as categorical, but it can also be used as continuous.

* Missing data in Ozone are related to Month, the probability for missing Ozone data is higher earlier in the year.

* The Chi-square test Solar.R and Month seems significant, but also throws a warning. So we cannot really be sure about the results. Probably also because there are only few missings in Solar.R. Tests for the other variables do not show a relation with the probability of missing data in Solar.R

Based on the univariate analyses we may conclude that the missing data are not Missing Completely at Random (not-MCAR). There are other measured variable related to the probability of missing data. 



```{r}
# Univariate tests for Ozone
t.test(Solar.R ~ ROzone, data = airqualitym)
t.test(Wind ~ ROzone, data = airqualitym)
t.test(Temp ~ ROzone, data = airqualitym)
t.test(Month ~ ROzone, data = airqualitym)
t.test(Day ~ ROzone, data = airqualitym)
chisq.test(airqualitym$ROzone, airqualitym$Month)


```


```{r}
# Univariate tests for Solar.R
t.test(Ozone ~ RSolar.R, data = airqualitym)
t.test(Wind ~ RSolar.R, data = airqualitym)
t.test(Temp ~ RSolar.R, data = airqualitym)
t.test(Month ~ RSolar.R, data = airqualitym)
t.test(Day ~ RSolar.R, data = airqualitym)
chisq.test(airqualitym$RSolar.R, airqualitym$Month)


```




**b. Evaluate the missing data mechanism for the airquality data with a multivariate test. What are your conclusions?**

First, investigate which variables have missing data in the `airquality` data: Ozone and Solar.R 

```{r}
summary(airquality)
mice::md.pattern(airquality)
```

Create missing data indicators for these two variables. Additional, we can also make one missing indicator for any missing values. Note that this is only useful if we have at least some variables that have no missing data. 

```{r}
airqualitym <- airquality %>%
  mutate(ROzone = is.na(Ozone),
         RSolar.R = is.na(Solar.R),
         Rind = is.na(Ozone) | is.na(Solar.R))

```

Do a logistic regression analysis for each of the missing data indicators. Both Temp and Month seem to be related to the missing values in Ozone. There are no measured variables related to the missing values in Solar.R. Based on these results we can conclude that the missing values in the airquality dataset are not-MCAR.

```{R}
glm(ROzone ~ Solar.R + Wind + Temp + Month + Day, data = airqualitym, family = "binomial") %>% summary
glm(RSolar.R ~ Ozone + Wind + Temp + Month + Day, data = airqualitym, family = "binomial") %>% summary

# analysis for the indicator of overall missing data
glm(Rind ~ Wind + Temp + Month + Day, data = airqualitym, family = "binomial") %>% summary

```

# Multiple imputation

## Solution 5: multiple imputation in `mice`

**a. How many imputed datasets are generated?**

The output returned from the mice functions states: "Number of multiple imputations: 5". 

```{R}
imp <- mice(nhanes2, seed = 1234)
imp
```

**b. How many sets of results are generated** 

The `with` function can be used to automatically analyze all imputed datasets. The analysis results are stored as `fit`. There are 5 sets of results generated.

```{R}
fit <- with(imp, lm(bmi ~ age + hyp + chl))
fit
```

**c. What are the most relevant predictors for bmi?**

The most relevant predictors are age and cholesterol. Older respondents have a lower bmi, whereas cholesterol is positively related (higher cholesteral means a higher bmi). 

```{R}
combi <- pool(fit)
summary(combi) %>% knitr::kable(digits = 3)

``` 


## Solution 6: multiple imputation model and convergence

**a. Adjust the predictor matrix so that the variables that have more than 50% missing values are excluded as predictors for the imputation.** 

First create the predictor matrix for the `boys` dataset with the `make.predictorMatrix()` function in `mice`. 

```{r}
pred <- make.predictorMatrix(boys)
```

Inspect the boys dataset, and find out what variables have more than 50% missing values. These are "gen", "phb", and "tv". 

```{r}
colMeans(is.na(boys))

```

Now, exclude variables "gen", "phb", and "tv" as predictors from this matrix. Note that predictors are in the columns.

```{r}
pred[,c("gen", "phb", "tv")] <- 0
pred
```

Perform multiple imputation on the `boys` data with the predictor matrix designed at assignment 6a with 10 imputations and 10 iterations.

```{r}
imp <- mice(boys, m = 10, maxit = 10, predictorMatrix = pred)
```

**b. What methods are used for the imputation of each variable and explain why these are used.**

* For "hgt", "wgt", "bmi", "hc" and "tv" the "pmm" (predictive mean matching) method is used. This is the default for continuous variables and the variables indicated are all continuous. 
* For "gen" and "phb" the "polr" method is used, which is the default for ordinal variables. The imputation function is a proportional odds model.
* For "reg" the "polyreg" method is used. This method is de default for unordered nominal variables and is the polytomous logistic regression. 


```{r}
imp$method
``` 

**c. Inspect the iteration plots. What are your observations?**

```{r}
plot(imp)
``` 


The iteration plots for the variables "hc", "gen", "phb", "tv", and "reg" all show crossing interacting lines that are more or less centered around. For the variables "hgt", "wgt" and "bmi" the lines to not cross as much and some lines are below the mean for all iterations, whereas other lines are above the mean for all iterations. So the iteration plots for "hgt", "wgt" and "bmi" do not show a very good convergence. 



**d. Adjust the predictor matrix such that hgt and wgt are not imputed by bmi, or vice versa, and that hgt and wgt are not used as predictors together with bmi. Why do you think that these changes are needed?**

Option 1:

First, we remove "hgt" and "wgt" as predictors for the imputaiton of "bmi". Second, we remove "bmi" as a predictor for all variables, to ensure that "bmi" is not used as predictor in a model together with "wgt" and "hgt". 

```{r}
pred1 <- pred
pred1["bmi", c("hgt", "wgt")] <- 0
pred1[,"bmi"] <- 0

```

Option 2:

We remove "hgt" and "wgt" as predictors for all variables, that way they won't be together in the model with "bmi" and they will not be used as predictors for imputing "bmi".

```{r}
pred2 <- pred
pred2[, c("hgt", "wgt")] <- 0
pred2[c("hgt", "wgt"), "bmi"] <- 0

```

Maybe there are other options?

These changes are needed, because bmi is calculated directly from height and weight. So using these variables together results in multi-collinearity problems with the model. 

**e. Use the adjusted predictor matrix to impute the boys dataset again, with 10 imputations and 10 iterations. Inspect the iteration plots again, do you see improvements for hgt, wgt and bmi?**


```{r}
imp1 <- mice(boys, m = 10, maxit = 10, pred = pred1)
plot(imp1)
imp2 <- mice(boys, m = 10, maxit = 10, pred = pred2)
plot(imp2)
```


**f. Another way to deal with the relation between hgt and wgt with bmi is to use "passive imputation" to impute the bmi variable. Adjust the method and predictor matrix in such a way that the bmi variable is passively imputed by hgt and wgt. Inspect the iteration plots again, do you see improvements for hgt, wgt and bmi?** 

The formula for bmi is $bmi =  \frac{weight}{height_{meters}^2}$. This formula is added as imputation method to compute missing bmi values, from the imputed weight and height values. We use the second option of the predictor matrix (option 1 is also possible), so that the updated bmi variable (from imputed hgt and wgt) is used as predictor for other variables. Option 1 might be preferred when bmi is used in the substantial analyses. 

```{r}

method <- imp$method
method["bmi"] <- "~I(wgt / (hgt/100)^2)"

imp <- mice(boys, m = 10, maxit = 10, pred = pred2, method = method)

plot(imp)
```




## Solution 7: FIML


**a. Repeat the example from the lecture on FIML and estimate the descriptive means for the `airquality` variables Ozone, Solar.R, Wind and Temp using FIML estimation.** 


```{r message=FALSE, warning=FALSE}
library("lavaan")
```

First specify the variables in the airquality data with the intercept code. It is possible to copy the code used in the Lecture, both give the same result.

The means are as follows: Ozone = 42.129; Solar.R = 185.932; Wind = 9.958; Temp = 77.882


```{r echo=TRUE}

model <- '
  #variance
  Ozone ~1 
  Solar.R ~1
  Wind ~1
  Temp ~1
  '
fit <- sem(model, data = airquality, missing = "fiml", meanstructure = TRUE)
summary(fit)
```


```{r echo=TRUE}
model <- '
  #variance
  Ozone ~~ Ozone
  Solar.R ~~ Solar.R
  Wind ~~ Wind
  Temp ~~ Temp
  '
fit <- sem(model, data = airquality, missing = "fiml", meanstructure = TRUE)
summary(fit)
```



**b. Now estimate a regression between `Ozone` and `Temp` using FIML estimation (Ozone as dependent variable), what is the coefficient for Temp?**

The coefficient for Temp = 2.429 (SE = 0.231).

```{r eval=TRUE, echo = TRUE}
model <- '
  Ozone ~ Temp 
  '
fit_fiml <- sem(model, data = airquality, missing = "fiml")
summary(fit_fiml, header = FALSE, fmi = TRUE)
```

**c. What is the Fraction of missing information in this regression analysis?**

The fraction of missing information reported for this analysis is shown in the last column and is equal to 0.24.


**d. Add the other variables in the data as auxiliary variables to estimate the regression between Ozone and Temp. Did the coefficient for Temp change?**

The coefficient for Temp changed to 2.354 (SE = 0.225). 

```{r echo=TRUE, message=FALSE, warning=FALSE}
model_aux <- '
  Ozone ~ Temp 
  Temp ~~ Wind + Solar.R + Month + Day
  Ozone ~~ Wind + Solar.R + Month + Day
  Wind ~~ Solar.R + Month + Day
  Solar.R ~~ Month + Day
  Month ~~ Day
  '
auxfit <- sem(model = model_aux, missing = "fiml", data = airquality)

summary(auxfit)
```


## Solution 8: Longitudinal missing data

**a. Which variables have missing data and how many?**

First select the variables: "id", "trt", "age", "sex", "cbcl1", "yc1", "yc2" and "yc3" from the `fdd` data for this assignment.

```{r} 

fdd1 <- fdd[,c("id", "trt", "age", "sex", "cbcl1", "yc1", "yc2", "yc3")]
summary(fdd1)

```

The fdd1 data only contains the selected variables for this assignment. The summary for this dataset shows that cbcl1 has 11 missing values, yc1 16, yc2 22 and yc3 24.

**b.  Analyze the data with a longitudinal model (yc1, yc2 and yc3 are the dependent repeated measurements), using time, age, sex and cbcl1 as predictor. What can you conclude about the ptsd scores? How many study participants are used in the analysis?**

To analyze the data with a longitudinal multilevel model (preferred, however repeated measurements model can be used too), we need to restructure the data into a long format. The repeated measurement indicator is the follow-up "time", and the outcome variable is a post-traumatic stress ("ptsd") score. 

```{r}
library(tidyr)

fdd1_long <- pivot_longer(fdd1, cols = c("yc1", "yc2", "yc3"), names_to = "time", values_to = "ptsd", names_prefix = "yc")

#note by using names_prefix, the prefix "yc" is removed from the time variable, so that it can be easily transformed to numeric if needed.

```

Apply the longitudinal model: use a linear multilevel regression with a random intercept for "id".

```{r}
library(lme4)
library(lmerTest)

model <- lmer(ptsd ~ time + age + sex + cbcl1+ (1|id), data = fdd1_long)
summary(model)

```

In total 33 persons are included in the analyses (out of the in total 52 participants in the data). Over time, the ptsd scores decrease.


**c. Perform a multilevel multiple imputation with only a random intercept. Use 50 imputations and 10 iterations for the imputations. Repeat the longitudinal analysis and compare the parameter estimates with the estimates from the previous answer. Describe your findings.**

The multilevel imputation should be performed on the long data format, so we use the restructured dataset to make a predictor matrix. In the predictor matrix we change the 1's in the "id" column to -2 to indicate that this is the level variable.

```{r}

pred <- make.predictorMatrix(fdd1_long)

#id should be indicated as the level variable, and the column should have a -2.
pred[2:7, "id"] <- -2

pred

```

Initialize the imputation model to change the method to "2l.pmm" to use a multilevel imputation method for the variables with missing data (trauma and ptsd). 

```{R}
library(miceadds)

ini <- mice(fdd1_long, m = 1, maxit = 0, pred = pred)
long_meth <- ini$method
long_meth[c("cbcl1", "ptsd")] <- "2l.pmm"
long_meth

```

Run the imputation, with 20 imputations and 10 iterations using the predictor matrix with -2 for "id",  and the "2l.pmm" methods for the variables that have missing values. 

```{r message=FALSE, warning=FALSE}

imp_long <- mice(fdd1_long, pred = pred, meth = long_meth, m = 20, maxit = 10)

```

Fit the model to each imputed dataset and pool the results. 

```{r}
library(broom.mixed)

fit <- with(imp_long, lmer(ptsd ~  time + age + sex + (1|id) ))
            
summary(pool(fit))
```


The results show a similar conclusion as for the previous answer; the ptsd scores decrease over time for all participants. However the coefficients seem less strong after imputation. One possible reason might be that the itention to treat analysis (analysis with imputations) reveals that mostly participants with less progress (less decrease in PTSD score) drop-out. Further investigations into the missing data and other information in the data may give us more insight into this. Note that now all 52 participants are included in the analyses (by calling `fit`).

